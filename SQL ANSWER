##flip gender details values.if existing gender is M change to F & vice-varsa.
-- create
CREATE TABLE EMPLOYEE (
  empId int,
  name varchar(15),
  dept varchar(10),
  gender bit
);

-- insert
INSERT INTO EMPLOYEE(empId,name,dept,gender) VALUES (1, 'Clark', 'Sales',0);
INSERT INTO EMPLOYEE(empId,name,dept,gender) VALUES (2, 'Dave', 'Accounting',1);
INSERT INTO EMPLOYEE(empId,name,dept,gender) VALUES (3, 'Ava', 'Sales',0);

-- fetch 
-- SELECT dept,string_agg(name,',') as employee_list FROM EMPLOYEE group by dept;
select *,case when gender=1 then 0 else 1 end as new_gender from EMPLOYEE
GO


import pandas as pd
data=[(1,'vinod',5000,'sales',1),(2,'vinay',6000,'sales',0),(3,'vikas',6000,'it',1)]
cols=['id','name','salary','dept','gender']
df=pd.DataFrame(data,columns=cols)
# df=df.groupby('dept',as_index=False).agg({'name':lambda x:','.join(sorted(x))}
print(df.head())
df['gender_new']=df['gender'].apply(lambda x:0 if x==1 else 1)
print(df.head())



# Initialize Spark session
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_date,date_add,col,collect_list,collect_set,concat_ws,when,lit
spark = SparkSession.builder.appName('Spark Playground').getOrCreate()

# Load the customers.csv dataset
# df = spark.read.format('csv').option('header', 'true').load('/samples/customers.csv')
data=[(1,'vinod',5000,'sales',1),(2,'vinay',6000,'sales',0),(3,'vikas',6000,'it',1)]
cols=['id','name','salary','dept','gender']
df=spark.createDataFrame(data,schema=cols)
# Show the first few rows of the DataFrame
# df.groupBy('dept').max('salary').show()
# df.groupBy('dept').agg(max(col('salary'))).show()
# df.groupBy('dept').agg(concat_ws('|',collect_set('name')).alias('employee_list')).show()
df=df.withColumn('new_gender',when(col('gender')==1,lit(0)).otherwise(lit(1)))
df.show(5)

# Display the DataFrame using the display() function.
df=df.withColumn('order_date',current_date()).withColumn('return_date',date_add(col('order_date'),10))
# df.show()

-----
###join based on id column from two tables first table have three one values and one null values,second table have three one value and two null values.
create table tab1 (id int)
create table tab2 (id int)

insert into tab1(id) values(1),(1),(1),(null),(null)
insert into tab2(id) values(1),(1),(1),(null),(null),(null)

select count(*) from tab1 inner join tab2
on tab1.id=tab2.id

select count(*) from tab1 left join tab2
on tab1.id=tab2.id

select count(*) from tab1 right join tab2
on tab1.id=tab2.id

select count(*) from tab1 full join tab2
on tab1.id=tab2.id


import pandas as pd
data1=[(1),(1),(1),(None),(None)]
cols=['id']
df1=pd.DataFrame(data=data1,columns=cols)
data2=[(1),(1),(1),(None),(None),(None)]
cols=['id']
df2=pd.DataFrame(data=data2,columns=cols)
print(df1.head(20))
print(df2.head(20))

df=pd.merge(df1,df2,how='inner',left_on='id',right_on='id')
print(df.head(20))

df=pd.merge(df1,df2,how='left',left_on='id',right_on='id')
print(df.head(20))

df=pd.merge(df1,df2,how='right',left_on='id',right_on='id')
print(df.head(20))

df=pd.merge(df1,df2,how='outer',left_on='id',right_on='id')
print(df.head(20))


# Initialize Spark session
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_date,date_add,col,collect_list,collect_set,concat_ws,when,lit
spark = SparkSession.builder.appName('Spark Playground').getOrCreate()

#create data-frame
data=[(1,),(1,),(1,),(None,),(None,)]
cols=['id']
df1=spark.createDataFrame(data,schema=cols)
data=[(1,),(1,),(1,),(None,),(None,),(None,)]
df2=spark.createDataFrame(data,schema=cols)
# apply all type of joins
print(df1.join(df2,on=(df1.id==df2.id),how='inner').count())
print(df1.join(df2,on=(df1.id==df2.id),how='left').count())
print(df1.join(df2,on=(df1.id==df2.id),how='right').count())
print(df1.join(df2,on=(df1.id==df2.id),how='outer').count())
print(df1.join(df2,on=(df1.id==df2.id),how='left_semi').count())
print(df1.join(df2,on=(df1.id==df2.id),how='left_anti').count())



